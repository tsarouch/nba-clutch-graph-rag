{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bed60c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5c3e30",
   "metadata": {},
   "source": [
    "## Download playoff PBP and extract 1997 Finals (Bulls–Jazz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75b2ee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from io import BytesIO, TextIOWrapper\n",
    "from urllib.request import urlopen\n",
    "import tarfile\n",
    "import csv\n",
    "from typing import Union, Sequence, Optional, List\n",
    "\n",
    "# fetch paths\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "import config\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Import config from project root. Adjust the path below if running elsewhere.\n",
    "import sys\n",
    "# Assume the notebook sits in `notebooks/` and config.py is in repo root:\n",
    "repo_root = Path(\"..\").resolve()\n",
    "sys.path.append(str(repo_root))\n",
    "import config\n",
    "\n",
    "# Paths from config\n",
    "RAW_DIR = Path(config.DATA_RAW_DIR)\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROCESSED_DIR = Path(config.DATA_PROCESSED_DIR)\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RAW_PLAYOFFS = Path(config.DATA_RAW_PLAYOFFS)\n",
    "RAW_FINALS = Path(config.DATA_RAW_FINALS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22d6acf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved playoffs CSV → /Users/charilaostsarouchas/Documents/Harris/04_Blueprints/agentic_ai/20250820_Bulls_Highlights_Retrieval/data/raw/pbp_1996_1997_playoffs.csv (rows=32,083)\n",
      "Saved Finals CSV → /Users/charilaostsarouchas/Documents/Harris/04_Blueprints/agentic_ai/20250820_Bulls_Highlights_Retrieval/data/raw/pbp_1997_finals_chi_uta.csv (games=6, rows=2,608)\n",
      "Detected GAME_IDs: [49600083, 49600084, 49600085, 49600086, 49600087, 49600088]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_nba_data(\n",
    "    path: Union[Path, str],\n",
    "    seasons: Union[Sequence, int] = (1996,),\n",
    "    data: Union[Sequence, str] = (\"nbastats\",),\n",
    "    seasontype: str = \"po\",\n",
    "    league: str = \"nba\",\n",
    "    in_memory: bool = True,\n",
    "    use_pandas: bool = True\n",
    ") -> Optional[Union[List, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Load NBA play-by-play archives from the public GitHub dataset:\n",
    "    https://github.com/shufinskiy/nba_data\n",
    "\n",
    "    Returns a concatenated pandas DataFrame if (in_memory & use_pandas) is True.\n",
    "    Otherwise writes archives to `path`.\n",
    "    \"\"\"\n",
    "    if isinstance(path, str):\n",
    "        path = Path(path).expanduser()\n",
    "    if isinstance(seasons, int):\n",
    "        seasons = (seasons,)\n",
    "    if isinstance(data, str):\n",
    "        data = (data,)\n",
    "\n",
    "    if seasontype == \"rg\":\n",
    "        need_data = [f\"{d}_{s}\" for d in data for s in seasons]\n",
    "    elif seasontype == \"po\":\n",
    "        need_data = [f\"{d}_{seasontype}_{s}\" for d in data for s in seasons]\n",
    "    else:\n",
    "        need_data = [f\"{d}_{s}\" for d in data for s in seasons]\n",
    "        need_data += [f\"{d}_{seasontype}_{s}\" for d in data for s in seasons]\n",
    "\n",
    "    with urlopen(\"https://raw.githubusercontent.com/shufinskiy/nba_data/main/list_data.txt\") as resp:\n",
    "        v = resp.read().decode(\"utf-8\")\n",
    "    name_v = [line.split(\"=\")[0] for line in v.split(\"\\n\") if \"=\" in line]\n",
    "    element_v = [line.split(\"=\")[1] for line in v.split(\"\\n\") if \"=\" in line]\n",
    "\n",
    "    need_name = [name for name in name_v if name in need_data]\n",
    "    need_element = [element for (name, element) in zip(name_v, element_v) if name in need_data]\n",
    "\n",
    "    if in_memory and use_pandas:\n",
    "        table = pd.DataFrame()\n",
    "    elif in_memory:\n",
    "        table = []\n",
    "    else:\n",
    "        table = None\n",
    "\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for name, url in zip(need_name, need_element):\n",
    "        with urlopen(url) as response:\n",
    "            if response.status != 200:\n",
    "                raise RuntimeError(f\"Failed to download: {url} (HTTP {response.status})\")\n",
    "            content = response.read()\n",
    "\n",
    "        if in_memory:\n",
    "            with tarfile.open(fileobj=BytesIO(content), mode=\"r:xz\") as tar:\n",
    "                csv_name = f\"{name}.csv\"\n",
    "                member = tar.getmember(csv_name)\n",
    "                f = tar.extractfile(member)\n",
    "                if use_pandas:\n",
    "                    df_part = pd.read_csv(f)\n",
    "                    df_part[\"__archive_name\"] = name\n",
    "                    table = pd.concat([table, df_part], axis=0, ignore_index=True)\n",
    "                else:\n",
    "                    reader = csv.reader(TextIOWrapper(f, encoding=\"utf-8\"))\n",
    "                    for row in reader:\n",
    "                        table.append(row)\n",
    "        else:\n",
    "            archive_path = path / f\"{name}.tar.xz\"\n",
    "            archive_path.write_bytes(content)\n",
    "\n",
    "    return table\n",
    "\n",
    "# Download 1996–97 playoffs\n",
    "df_playoffs = load_nba_data(\n",
    "    path=RAW_DIR,\n",
    "    seasons=(1996,),       # 1996 -> 1996–97 season\n",
    "    data=(\"nbastats\",),\n",
    "    seasontype=\"po\",\n",
    "    league=\"nba\",\n",
    "    in_memory=True,\n",
    "    use_pandas=True\n",
    ")\n",
    "\n",
    "# Basic checks and save full playoffs CSV\n",
    "assert \"GAME_ID\" in df_playoffs.columns, \"GAME_ID missing in downloaded data\"\n",
    "df_playoffs.sort_values([\"GAME_ID\",\"PERIOD\",\"EVENTNUM\"], inplace=True, ignore_index=True)\n",
    "df_playoffs.to_csv(RAW_PLAYOFFS, index=False)\n",
    "print(f\"Saved playoffs CSV → {RAW_PLAYOFFS} (rows={len(df_playoffs):,})\")\n",
    "\n",
    "# Extract Bulls–Jazz games (the 1997 Finals)\n",
    "def collect_team_abbrevs(df: pd.DataFrame) -> pd.Series:\n",
    "    TEAM_COL_CANDIDATES = [\n",
    "        \"PLAYER1_TEAM_ABBREVIATION\",\"PLAYER2_TEAM_ABBREVIATION\",\"PLAYER3_TEAM_ABBREVIATION\",\n",
    "        \"TEAM_ABBREVIATION\",\"PLAYER1_TEAM_CITY\",\"PLAYER2_TEAM_CITY\",\"PLAYER3_TEAM_CITY\",\n",
    "    ]\n",
    "    DESC_COLS = [\"HOMEDESCRIPTION\",\"VISITORDESCRIPTION\",\"NEUTRALDESCRIPTION\"]\n",
    "\n",
    "    team_sets = {}\n",
    "    has_cols = [c for c in TEAM_COL_CANDIDATES if c in df.columns]\n",
    "    for gid, g in df.groupby(\"GAME_ID\"):\n",
    "        teams = set()\n",
    "        for c in has_cols:\n",
    "            vals = g[c].dropna().astype(str).str.upper().str.strip()\n",
    "            teams.update([v for v in vals if re.fullmatch(r\"[A-Z]{2,4}\", v)])\n",
    "        if not teams:\n",
    "            for dcol in [c for c in DESC_COLS if c in df.columns]:\n",
    "                txt = \" \".join(g[dcol].dropna().astype(str).tolist()).upper()\n",
    "                if \" CHI \" in f\" {txt} \" or \" CHI.\" in txt or \" CHI,\" in txt:\n",
    "                    teams.add(\"CHI\")\n",
    "                if \" UTA \" in f\" {txt} \" or \" UTAH\" in txt or \" UTA,\" in txt:\n",
    "                    teams.add(\"UTA\")\n",
    "        team_sets[gid] = teams\n",
    "    return pd.Series(team_sets, name=\"teams\")\n",
    "\n",
    "def finals_game_ids_1997(df: pd.DataFrame) -> List[str]:\n",
    "    team_sets = collect_team_abbrevs(df)\n",
    "    return sorted([gid for gid, teams in team_sets.items() if {\"CHI\",\"UTA\"}.issubset(teams)])\n",
    "\n",
    "df_9697 = df_playoffs[df_playoffs[\"__archive_name\"].str.contains(\"_po_1996$\")].copy()\n",
    "finals_gids = finals_game_ids_1997(df_9697)\n",
    "assert len(finals_gids) > 0, \"No CHI–UTA playoff games found in 1996–97\"\n",
    "\n",
    "df_finals = df_9697[df_9697[\"GAME_ID\"].isin(finals_gids)].copy()\n",
    "df_finals.to_csv(RAW_FINALS, index=False)\n",
    "print(f\"Saved Finals CSV → {RAW_FINALS} (games={len(set(finals_gids))}, rows={len(df_finals):,})\")\n",
    "print(\"Detected GAME_IDs:\", sorted(set(finals_gids)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3407fb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-gr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
